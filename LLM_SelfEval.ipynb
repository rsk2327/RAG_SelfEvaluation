{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67379e10-debf-4af3-8382-6a5eb3ef2ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from io import StringIO \n",
    "import json\n",
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a739c4f8-8c97-44e6-b392-2fe2684807a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = \"./test_data\"\n",
    "QUESTION_FILE =  \"document_questions.xlsx\"\n",
    "RAW_DATA_FOLDER = \"raw_text\"\n",
    "MODEL = 'gpt-3.5-turbo-0301'\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92b9311-d2c7-49a0-944e-528d02ff572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(os.path.join(BASE_FOLDER, question_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33686f3f-adb7-4486-9078-f366ce6f1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What is meant by \"computational finance\"?'\n",
    "answer = 'Computational finance refers to the use of computational tools to craft models that provide insights into investor heterogeneity and dynamics in financial settings. Agent-based computational models view financial markets as interacting groups of learning, boundedly-rational agents. These models require computational tools when analytic solutions are impossible.'\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d826325-3a4b-42c4-93d9-3cc70bcbfb7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e228783c-712b-43cf-beb9-ad1069b74118",
   "metadata": {},
   "source": [
    "# Option 1\n",
    "\n",
    "- Uses Langchain's internal JSON output parser\n",
    "- Forces Enum class type on grade output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e013f227-b3ff-4934-a30b-6f4e2fdfcc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class gradeEnum(str,Enum):\n",
    "    correct = \"correct\"\n",
    "    incorrect = \"incorrect\"\n",
    "    \n",
    "class LLMEvalResult(BaseModel):\n",
    "    grade: gradeEnum = Field(description=\"Final grade label. Accepted labels : Correct, Incorrect\")\n",
    "    description: str = Field(description=\"Explanation of why the specific grade was assigned. Must be concise. Not more than 2 sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cb26310-f17c-430b-bf75-b2bd83efbbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_parser = JsonOutputParser(pydantic_object=LLMEvalResult)\n",
    "\n",
    "qa_eval_prompt_text = \"\"\"\n",
    "You are a teacher evaluating a test. \n",
    "You are provided with a question along with an answer for the question written by a student. Evaluate the question-answer pair and provide feedback.\n",
    "{format_instructions}\n",
    "Question : {question}\n",
    "Answer : {answer}\n",
    "\"\"\"\n",
    "\n",
    "qa_eval_prompt = PromptTemplate(\n",
    "    template=qa_eval_prompt_text,\n",
    "    input_variables=[\"question\",\"answer\"],\n",
    "    partial_variables={\"format_instructions\": json_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ab20650-bd4a-4f64-9c9e-987c4205582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_eval_chain = qa_eval_prompt | llm | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56824f1e-f497-4852-8a74-20aa08cca3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grade': 'Correct',\n",
       " 'description': 'The answer provides a clear and concise explanation of computational finance, highlighting the use of computational tools in crafting models for financial settings.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_eval_chain.invoke({'question':question, 'answer':answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c876cc4f-6a91-4634-ae69-a07bd6b96217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1165"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the length of the prompt\n",
    "len(qa_eval_prompt.invoke({'question':\"\", 'answer':\"\"}).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8516ba3d-f548-4c04-ab36-3876ac2564ab",
   "metadata": {},
   "source": [
    "# Option 2 \n",
    "- No explicit constraint on grade value output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0cff430-9caf-444f-b080-90f314f19076",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_eval_prompt_text = \"\"\"\n",
    "You are a teacher evaluating a test. \n",
    "You are provided with a question along with an answer for the question written by a student. Evaluate the question-answer pair and provide feedback in the following JSON format :\n",
    "{{\"grade\" : Final grade label. Accepted labels : Correct, Incorrect,\n",
    "\"description\" : Explanation of why the specific grade was assigned. Must be concise. Not more than 2 sentences\n",
    "}}\n",
    "\n",
    "\n",
    "Question : {question}\n",
    "Answer : {answer}\n",
    "\"\"\"\n",
    "\n",
    "qa_eval_prompt = ChatPromptTemplate.from_template(qa_eval_prompt_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9a8a09b-0c30-4fdc-9de8-2d9d8aad542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_eval_chain = qa_eval_prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "231a94d6-61c0-4d1b-adc8-c8a306bb45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_eval_output = llm_eval_chain.invoke({'question':question, 'answer':answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ff776f-84fe-4d58-bbe6-87c77f90aca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grade': 'Correct',\n",
       " 'description': 'The answer provides a clear and accurate definition of computational finance, explaining its use of computational tools in financial settings and the concept of agent-based computational models.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.load(StringIO(llm_eval_output.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b085fd56-3425-49a6-806e-a2dda2c2a1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the length of the prompt\n",
    "len(qa_eval_prompt.invoke({'question':\"\", 'answer':\"\"}).messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1aacb2-8b32-4746-8ded-98fce3bf2eef",
   "metadata": {},
   "source": [
    "# Bulk Eval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9bd50-9c13-44eb-8d4b-df04fa95b101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
